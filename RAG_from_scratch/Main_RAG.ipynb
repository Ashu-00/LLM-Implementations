{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "STEPS-\n",
        "1. Open a pdf file.\n",
        "2. Format the text.\n",
        "3. Embed and turn the chunks of text into embeddings.\n",
        "4. Build retrieval system based on Vector search via query.\n",
        "5. Create a prompt that incorporates the retrieved text.\n",
        "6. Generate an answer to the query."
      ],
      "metadata": {
        "id": "EhhRo2zIo0O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "n4VmBRFjuDYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8T-Jnebn3aC"
      },
      "outputs": [],
      "source": [
        "#import pdf document\n",
        "import fitz\n",
        "from tqdm.auto import tqdm\n",
        "pdf_path=\"/content/simple-local-rag/human-nutrition-text.pdf\"\n",
        "\n",
        "def process_text(text):\n",
        "  return text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "def open_pdf(pdf_path):\n",
        "  pdf= fitz.open(pdf_path)\n",
        "  page_text=[]\n",
        "  for page_num, page in tqdm(enumerate(pdf)):\n",
        "    text= process_text(page.get_text())\n",
        "    page_text.append(\n",
        "        {\"page_number\": page_num - 41,\n",
        "                                \"page_char_count\": len(text),\n",
        "                                \"page_word_count\": len(text.split(\" \")),\n",
        "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "                                \"page_token_count\": len(text) / 4,  # assuming 4 tokens per character\n",
        "                                \"text\": text}\n",
        "        )\n",
        "  return page_text\n",
        "\n",
        "page_text_list=open_pdf(pdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(page_text_list)"
      ],
      "metadata": {
        "id": "f-N97Mp5wDUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.sample(page_text_list, k=2)"
      ],
      "metadata": {
        "id": "q6AlFbxIvtWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to sentences using spaCy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "sentenser=English()\n",
        "\n",
        "sentenser.add_pipe(\"sentencizer\")\n",
        "\n",
        "for page in tqdm(page_text_list):\n",
        "  page[\"sentences\"]=list(sentenser(page[\"text\"]).sents)\n",
        "\n",
        "  page[\"sentences\"]=[str(sent) for sent in page[\"sentences\"]]\n",
        "\n",
        "  page[\"sentence_count\"]=len(page[\"sentences\"])\n",
        "\n",
        "random.sample(page_text_list,k=1)\n"
      ],
      "metadata": {
        "id": "c-PCJSLPLQlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(page_text_list)\n",
        "df.describe().round(1)"
      ],
      "metadata": {
        "id": "W8eAztN6OD7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text(1 page all sentences) > chunk(10 sentences) > sentence\n",
        "\n",
        "def split_list(givenList, splitsize):\n",
        "  return [givenList[i:i+splitsize] for i in range(0, len(givenList), splitsize)]\n",
        "\n",
        "chunk_size=10\n",
        "for item in tqdm(page_text_list):\n",
        "  item[\"chunks\"]=split_list(item[\"sentences\"], chunk_size)\n",
        "  item[\"chunk_count\"]=len(item[\"chunks\"])\n",
        "\n",
        "random.sample(page_text_list, k=1)"
      ],
      "metadata": {
        "id": "cv5-_Oz4Ug1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#joining all chunk sentences\n",
        "chunk_list=[]\n",
        "\n",
        "\n",
        "for item in page_text_list:\n",
        "  for chunk in item[\"chunks\"]:\n",
        "    chunk_dict={}\n",
        "    chunk_dict[\"page_number\"]=item[\"page_number\"]\n",
        "\n",
        "    joined_chunk=\"\".join(chunk).replace(\"  \", \" \").strip()\n",
        "    joined_chunk=re.sub( r'\\.([A-Z])', r'. \\1', joined_chunk)\n",
        "    chunk_dict[\"chunk\"]=joined_chunk\n",
        "\n",
        "    chunk_dict[\"chunk_char_count\"]=len(joined_chunk)\n",
        "    chunk_dict[\"chunk_word_count\"]=len(joined_chunk.split(\" \"))\n",
        "    chunk_dict[\"chunk_char_count\"]=len(joined_chunk)/4\n",
        "    chunk_list.append(chunk_dict)\n",
        "\n",
        "len(chunk_list)\n"
      ],
      "metadata": {
        "id": "yTpm6eDNDaP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing small chunks\n",
        "min_tokens=30\n",
        "\n",
        "df=pd.DataFrame(chunk_list)\n",
        "big_chunks_list=df[df[\"chunk_char_count\"]>min_tokens].to_dict(orient=\"records\")\n",
        "len(big_chunks_list)\n"
      ],
      "metadata": {
        "id": "6HQYnjCwzvN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "embed_model=SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
        "\n",
        "for item in tqdm(big_chunks_list):\n",
        "  item[\"embeddings\"]=embed_model.encode(item[\"chunk\"])"
      ],
      "metadata": {
        "id": "KavL56ai0npy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save embeddings to file\n",
        "text_chunks_and_embeddings_df = pd.DataFrame(big_chunks_list)\n",
        "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
        "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
      ],
      "metadata": {
        "id": "5R-5lzqv8Cxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END OF EMBEDDINGS PART\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "PART-2 Retrieval"
      ],
      "metadata": {
        "id": "IPPZMVGm9DfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
        "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
        "text_chunks_and_embedding_df_load.head()"
      ],
      "metadata": {
        "id": "FHEvKs-u82Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "text_chunks_and_embedding_df_load[\"embeddings\"]= text_chunks_and_embedding_df_load[\"embeddings\"].apply(lambda x: np.fromstring(x.strip(\"[]\"),sep=\" \"))\n",
        "\n",
        "new_chunk_list= text_chunks_and_embedding_df_load.to_dict(orient=\"records\")\n",
        "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df_load[\"embeddings\"].tolist()), dtype=torch.float32).to(device)\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "Mcsb1SQy9C1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"Protein Deficiency\"\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "embed_model=SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
        "\n",
        "qembed=embed_model.encode(query ,convert_to_tensor=True)\n",
        "\n",
        "#Cosine similarity must be used for text similarity due to -1 to 1 output\n",
        "#We use dot product as embedding model returns normalized output anyways\n",
        "#which will give similar results\n",
        "dot_prod=util.dot_score(a=qembed, b=embeddings)[0]\n",
        "\n",
        "print(dot_prod.shape)"
      ],
      "metadata": {
        "id": "jdoOTsCaHF45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def print_wrapped(text, wrap_length=80):\n",
        "    wrapped_text = textwrap.fill(text, wrap_length)\n",
        "    print(wrapped_text)"
      ],
      "metadata": {
        "id": "Cxf9vGbaQYlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query\", query)\n",
        "top_results = torch.topk(dot_prod, k=5)\n",
        "\n",
        "for score,indx in zip(top_results[0],top_results[1]):\n",
        "  print(\"score\",score)\n",
        "  print_wrapped(new_chunk_list[indx][\"chunk\"])\n",
        "  print(\"Page\", new_chunk_list[indx][\"page_number\"])\n",
        "  print(\"-\"*10,\"\\n\")\n"
      ],
      "metadata": {
        "id": "3J3fD1oTQolW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieval_pipeline(\n",
        "      query,\n",
        "      embeddings=embeddings,\n",
        "      embed_model=embed_model,\n",
        "      top_k=5,\n",
        "  ):\n",
        "  qembed=embed_model.encode(query ,convert_to_tensor=True)\n",
        "  dot_prod=util.dot_score(a=qembed, b=embeddings)[0]\n",
        "\n",
        "  top_results = torch.topk(dot_prod, k=top_k)\n",
        "  return top_results[0],top_results[1]\n",
        "\n",
        "def print_retrieved_chunks(\n",
        "        query,\n",
        "        embeddings=embeddings,\n",
        "        embed_model=embed_model,\n",
        "        top_k=5,\n",
        "  ):\n",
        "  score,indx=retrieval_pipeline(query,embeddings,embed_model,top_k)\n",
        "  print(\"Query\", query)\n",
        "  for score,indx in zip(score,indx):\n",
        "    print(\"score\",score)\n",
        "    print_wrapped(new_chunk_list[indx][\"chunk\"])\n",
        "    print(\"Page\", new_chunk_list[indx][\"page_number\"])\n"
      ],
      "metadata": {
        "id": "po7mFYjrSRhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_retrieved_chunks(\"Kwashiorkor\")"
      ],
      "metadata": {
        "id": "xjOsZ9n0U4tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "KSq-esdvVL33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
        "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
        "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
      ],
      "metadata": {
        "id": "4djJc9ZadyKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if gpu_memory_gb < 5.1:\n",
        "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
        "elif gpu_memory_gb < 8.1:\n",
        "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
        "    use_quantization_config = True\n",
        "    model_id = \"google/gemma-2b-it\"\n",
        "elif gpu_memory_gb < 19.0:\n",
        "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
        "    use_quantization_config = False\n",
        "    model_id = \"google/gemma-2b-it\"\n",
        "elif gpu_memory_gb > 19.0:\n",
        "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
        "    use_quantization_config = False\n",
        "    model_id = \"google/gemma-7b-it\"\n",
        "\n",
        "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
        "print(f\"model_id set to: {model_id}\")"
      ],
      "metadata": {
        "id": "V9TrAQRndom-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_api=userdata.get('HF_api')"
      ],
      "metadata": {
        "id": "-4sc0I6yLWvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.utils import is_flash_attn_2_available\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,token=hf_api)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, token=hf_api)\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                         bnb_4bit_compute_dtype=torch.float16)\n",
        "\n",
        "\n",
        "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
        "  attn_implementation = \"flash_attention_2\"\n",
        "else:\n",
        "  attn_implementation = \"sdpa\"\n",
        "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id,device=\"cuda\")\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
        "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
        "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
        "                                                 low_cpu_mem_usage=True, # use full memory\n",
        "                                                 attn_implementation=attn_implementation, # which attention version to use\n",
        "                                                 device=\"cuda\")\n",
        "\n",
        "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU\n",
        "    llm_model.to(\"cuda\")\n",
        "\n"
      ],
      "metadata": {
        "id": "okfEOMTZIurI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model"
      ],
      "metadata": {
        "id": "82VH7jveJKln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ef get_model_mem_size(model: torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Get how much memory a PyTorch model takes up.\n",
        "\n",
        "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
        "    \"\"\"\n",
        "    # Get model parameters and buffer sizes\n",
        "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
        "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
        "\n",
        "    # Calculate various model sizes\n",
        "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
        "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
        "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
        "\n",
        "    return {\"model_mem_bytes\": model_mem_bytes,\n",
        "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
        "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
        "\n",
        "get_model_mem_size(llm_model)"
      ],
      "metadata": {
        "id": "EIUTPz7RMDnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
        "print(f\"Input text:\\n{input_text}\")\n",
        "\n",
        "# Create prompt template for instruction-tuned model\n",
        "dialogue_template = [\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": input_text}\n",
        "]\n",
        "\n",
        "# Apply the chat template\n",
        "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
        "                                       tokenize=False, # keep as raw text (not tokenized)\n",
        "                                       add_generation_prompt=True)\n",
        "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
      ],
      "metadata": {
        "id": "2He0q_q7eJL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok_ip=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "tok_op=llm_model.generate(\n",
        "    input_ids=tok_ip[\"input_ids\"],\n",
        "    attention_mask=tok_ip[\"attention_mask\"],\n",
        "    max_new_tokens=128,\n",
        "    )"
      ],
      "metadata": {
        "id": "7I0rHjQCf2La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=tokenizer.decode(tok_op[0], skip_special_tokens=True)\n",
        "output_replaced=output.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')\n",
        "print(f\"Output text:\\n{output}\")"
      ],
      "metadata": {
        "id": "_FE2qSEmgiFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qlist=[\n",
        "    \"How often should infants be breastfed?\",\n",
        "    \"What are symptoms of pellagra?\",\n",
        "    \"How does saliva help with digestion?\",\n",
        "    \"What is the RDI for protein per day?\",\n",
        "    \"water soluble vitamins\",\n",
        "]"
      ],
      "metadata": {
        "id": "ORE2r4UhhUSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "query = random.choice(query_list)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                              embeddings=embeddings)\n",
        "scores, indices"
      ],
      "metadata": {
        "id": "m19tlJOOhhh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AUGMENTATION\n",
        "\n",
        "def format_prompt(query, context_items):\n",
        "\n",
        "  context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
        "\n",
        "  base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
        "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
        "Don't return the thinking, only return the answer.\n",
        "Make sure your answers are as explanatory as possible.\n",
        "Use the following examples as reference for the ideal answer style.\n",
        "\\nExample 1:\n",
        "Query: What are the fat-soluble vitamins?\n",
        "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
        "\\nExample 2:\n",
        "Query: What are the causes of type 2 diabetes?\n",
        "Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
        "\\nExample 3:\n",
        "Query: What is the importance of hydration for physical performance?\n",
        "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
        "\\nNow use the following context items to answer the user query:\n",
        "{context}\n",
        "\\nRelevant passages: <extract relevant passages from the context here>\n",
        "User query: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "  base_prompt = base_prompt.format(context=context, query=query)\n",
        "\n",
        "    # Create prompt template\n",
        "    dialogue_template = [\n",
        "        {\"role\": \"user\",\n",
        "        \"content\": base_prompt}\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
        "                                          tokenize=False,\n",
        "                                          add_generation_prompt=True)\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "J958pD48hlEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = random.choice(query_list)\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# Get relevant resources\n",
        "scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                              embeddings=embeddings)\n",
        "\n",
        "# Create a list of context items\n",
        "context_items = [pages_and_chunks[i] for i in indices]\n",
        "\n",
        "# Format prompt with context items\n",
        "prompt = prompt_formatter(query=query,\n",
        "                          context_items=context_items)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "4FP0XsG4it7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(query,\n",
        "        temperature=0.7,\n",
        "        max_new_tokens=512,\n",
        "        format_answer_text=True,\n",
        "        return_answer_only=True):\n",
        "\n",
        "    # Get just the scores and indices of top related results\n",
        "    scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                                  embeddings=embeddings)\n",
        "\n",
        "    # Create a list of context items\n",
        "    context_items = [pages_and_chunks[i] for i in indices]\n",
        "\n",
        "    # Add score to context item\n",
        "    for i, item in enumerate(context_items):\n",
        "        item[\"score\"] = scores[i].cpu() # return score back to CPU\n",
        "\n",
        "    # Format the prompt with context items\n",
        "    prompt = prompt_formatter(query=query,\n",
        "                              context_items=context_items)\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate an output of tokens\n",
        "    outputs = llm_model.generate(**input_ids,\n",
        "                                 temperature=temperature,\n",
        "                                 do_sample=True,\n",
        "                                 max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Turn the output tokens into text\n",
        "    output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "    if format_answer_text:\n",
        "        # Replace special tokens and unnecessary help message\n",
        "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
        "\n",
        "    # Only return the answer without the context items\n",
        "    if return_answer_only:\n",
        "        return output_text\n",
        "\n",
        "    return output_text, context_items"
      ],
      "metadata": {
        "id": "RIzHpyZGi33X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = ask(query=query, temperature=1, max_new_tokens=256, return_answer_only=False)"
      ],
      "metadata": {
        "id": "mV-QgCmGjC--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_wrapped(answer)"
      ],
      "metadata": {
        "id": "qg7XYu84jUkN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}